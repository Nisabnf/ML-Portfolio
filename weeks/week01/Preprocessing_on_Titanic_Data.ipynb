{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "QS9QhesdNGTy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1. Load the Titanic dataset\n",
        "# ----------------------------\n",
        "data = pd.read_csv(\"/content/Titanic-Dataset.csv\")  # make sure train.csv is extracted from archive\n",
        "print(\"First 5 rows:\\n\", data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF_dGWKNNP_6",
        "outputId": "7bb83db2-9c29-44b5-91b5-e6ec1ce4f28a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows:\n",
            "    PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable = Survived\n",
        "y = data[\"Survived\"]\n",
        "X = data.drop(columns=[\"Survived\"])"
      ],
      "metadata": {
        "id": "ATMe2yIpNtz5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2. Identify columns\n",
        "# ----------------------------\n",
        "# Numerical features (we will scale them)\n",
        "numeric_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "\n",
        "# Categorical features (we will encode them)\n",
        "categorical_features = [\"Pclass\", \"Sex\", \"Embarked\"]"
      ],
      "metadata": {
        "id": "LU4eUQSpN0K-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3. Preprocessing steps\n",
        "# ----------------------------\n",
        "\n",
        "# For numeric: fill missing values with median, then scale\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # fill missing numbers\n",
        "    (\"scaler\", StandardScaler())                     # scale to mean=0, std=1\n",
        "])"
      ],
      "metadata": {
        "id": "eSzUb3bBORZJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For categorical: fill missing with most frequent, then one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),   # fill missing categories\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))     # convert to 0/1 columns\n",
        "])\n"
      ],
      "metadata": {
        "id": "p2KymwLyO1VP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine numeric + categorical transformers\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9tI__T7ePHEK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4. Build pipeline with model\n",
        "# ----------------------------\n",
        "# Logistic Regression is simple and interpretable\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", LogisticRegression(max_iter=500))\n",
        "])\n"
      ],
      "metadata": {
        "id": "ThIull_WPL0q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. Train/Test split for evaluation\n",
        "# ----------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOtseMOBPWwW",
        "outputId": "9fb7dfb6-9490-4e03-e9c0-50457a6f27d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7988826815642458\n"
          ]
        }
      ]
    }
  ]
}